---
title: "Chill unit models"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(here)
library(readr)
library(dplyr)
library(lubridate)
library(ggplot2)
library(magrittr)
library(tidyr)
library(kableExtra)


getHoursBelowSeven <- function(x) {
	return(ifelse(x < 7, 1, 0))
}

getRichardsonChillUnits <- function(x) {
	return(case_when(x < 1.5 ~ 0,
					 x >= 1.5 & x < 2.5 ~ 0.5,
					 x >= 2.5 & x < 9.2 ~ 1,
					 x >= 9.2 & x < 12.5 ~ 0.5,
					 x >= 12.5 & x < 16 ~ 0,
					 x >= 16 & x < 18 ~ -0.5,
					 x >= 18 ~ -1))
}

getUtahChillUnits <- function(x) {
	return(case_when(x < 1.11 ~ 0,
					 x >= 1.11 & x < 2.22 ~ 0.5,
					 x >= 2.22 & x < 8.89 ~ 1,
					 x >= 8.89 & x < 12.22 ~ 0.5,
					 x >= 12.22 & x < 15.56 ~ 0,
					 x >= 15.56 & x < 18.33 ~ -0.5,
					 x >= 18.33 ~ -1))
}

getChillingPortions <- function(x) {
	tk <- x + 273
	ftmprt <- round(1.6 * 277 *(tk - 277) / tk, digits = 13)
	sr <- exp(ftmprt)
	xi <- sr/(1+sr)

	aa <- (1.395*10^5) / (2.567*10^18)
	ee <- (1.28888*10^4) - (4.1535*10^3)
	
	xs <- aa * exp(ee/tk)
	
	ak <- (2.567*10^18) * exp(-1.28888*10^4 / tk)
	
	inters <- rep(0, length(x))
	intere <- rep(0, length(x))
	intere[1]	<- xs[1] - (xs[1] - inters[1]) * exp(-ak[1])
	## do i really have to cycle through row-wise?
	for(i in 2:length(x)){
		inters[i] <- ifelse(intere[(i-1)] < 1, intere[i - 1], intere[i - 1] - (intere[i - 1] * xi[i - 1]))
		intere[i] <- xs[i] - (xs[i] - inters[i]) * exp(-ak[i])
	}
	
	delta <- ifelse(intere < 1, 0, intere * xi)
	
	#return(delta)

	return(data.frame(tk = tk,
					  ftmprt = ftmprt,
					  sr = sr,
					  xi = xi,
					  xs = xs,
					  ak = ak,
					  inters = inters,
					  intere = intere,
					  delta = delta))

}

```

### Background
There are multiple ways of calculating chilling units for fruit and nut crops, each posing a 'better' way of quantifying the amount of chill accumulating during the winter season prior to budbreak. Historic, baseline model is the simple "Chill Units" calculation which gives a chill unit of 1 for every hour at or below 7 degrees C. Other models have gone into more depth, dividing up the temperature ranges and giving them weightings of effectiveness such as the <a href="http://fruitsandnuts.ucdavis.edu/Weather_Services/chilling_accumulation_models/about_chilling_units/">Utah</a> and <a href="http://harvest.com/support/calculations/">Richardson</a>, and variations on these. 

None of these models is a perfect predictor, the baseline 'below 7C' will give an indication if it is a warm or cool year, the Utah and Richardson models have a bit more finesse in relation to the effect on a plant's physiological state but do not deal well wih warmer coastal climates, nor with periodic warm and then cold spells.

The final model is the <a href="https://ucanr.edu/sites/fruittree/How-to_Guides/Dynamic_Model_-_Chill_Accumulation/">Dynamic Model</a>. This uses a function over a period of time at a temperature threshold to accumulate chilling portions.


### Ramblings

The chill is thought to break down starch into simple sugars which in turn creates osmodic pressure to drive the start of budbreak.



```{r data_import, echo=FALSE, include=FALSE}

# Exclude 2003 as that is an incomplete year

met_data <- 
	read_csv(here("input/MetWatch Export.csv"),
			 na = c('-',NA)) %>%
	mutate(date = dmy_hm(`Stop Date`),
		   doy = yday(date),
		   year = year(date)) %>%
	rename(StationID = 1,
		   Temperature = 4,
		   RH = 5, 
		   Rainfall = 6, 
		   Radiation = 7, 
		   WindSpeed = 8) %>%
	select(StationID, date, everything(), -2, -3) %>%
	filter((month(date) >= 5 & month(date) <= 9) & year != 2003) %>%
	group_by(StationID, year)


```
### Niwa Data



Expected number of hours between May 1st and Septmeber 30th;
```{r, echo = FALSE, message=FALSE}

as.numeric(difftime(ymd("2021-10-01"), ymd("2021-05-01"), "days")) * 24
```

Check how many data points are available for each station by year

```{r, echo = FALSE, message = FALSE}
met_data %>%
	summarise(n = n()) %>%
	pivot_wider(names_from = year, values_from = n, values_fill = NA_integer_) %>%
	knitr::kable()
```
  
Now what about NA's?

```{r, echo = FALSE, message = FALSE}
met_data %>%
	filter(is.na(Temperature)) %>%
	summarise(nas = n()) %>%
	pivot_wider(names_from = year, values_from = nas, values_fill = 0) %>%
	knitr::kable()

```

Any station with more than a weeks missing data should be excluded.
Need to remove NAs first

``` {r, echo =FALSE, message = FALSE}

exclusions <- 
	met_data %>%
	filter(is.na(Temperature)) %>%
	summarise(missing_data_points = n()) %>%
	filter(missing_data_points > (24*7))

exclusions %>%	
	knitr::kable()

```

```{r, echo = FALSE}

# need to remove NAs for functions to work
met_data %<>%
	filter(!is.na(Temperature)) %>%
	group_by(StationID, year) %>%
	mutate(hours_below_seven = cumsum(getHoursBelowSeven(Temperature)),
		   richardson_units = cumsum(getRichardsonChillUnits(Temperature)),
		   utah_units = cumsum(getUtahChillUnits(Temperature)),
		   chilling_portions = cumsum(getChillingPortions(Temperature)))

```

### Hours below 7Â°C

An example of this model on Te Puke yearly data, 1st May to 1st September.


```{r, echo=FALSE}

doy_ticks <- 
	met_data%>%
	filter(StationID == "TPK" & (month(date) >= 5 & month(date) <= 9)) %$%
	pretty(x = yday(date), n = 7)

doy_labels <- format(as.Date(doy_ticks, origin), "%b-%d")

met_data %>%
	filter(StationID == "TPK" & year(date) != 2017) %>%
	group_by(year(date), StationID) %>%
	mutate(chilling_hours = cumsum(case_when(Temperature < 7 ~ 1,
											 Temperature >= 7 ~ 0,
											 is.na(Temperature) ~ 0)),
		   doy = yday(date)) %>%
	ggplot(aes(doy, chilling_hours, colour = factor(year(date)))) +
		geom_line() +
		scale_x_continuous(breaks = doy_ticks, labels = doy_labels) +
		labs(x = "Date", y = "Cumulative chilling hours", caption = "data for 2017 is excluded due to sensor error", colour = "Year") + 
		theme_bw() + 
		theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
				
```

## Richardson Chill Units

Example of a Richardson chill graph for Te Puke.

```{r, echo=FALSE}

met_data %>%
	filter(StationID == "TPK" & year != 2017) %>%
	ggplot() +
		geom_line(aes(doy, richardson_units, colour = factor(year))) +
		labs(x = "Date", y = "Richardson Chill Units", caption = "data for 2017 is exlcuded due to sensor error", colour = "Year") +
		scale_x_continuous(breaks = doy_ticks, labels = doy_labels) +
		theme_bw() + 
		theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
		ylim(-200, 2000)

```

### Utah model

Example graph for the base Utah model using Te Puke data.

```{r, echo=FALSE}
met_data %>%
	filter(StationID == "TPK" & year != 2017) %>%
	ggplot() +
		geom_line(aes(doy, utah_units, colour = factor(year))) +
		labs(x = "Date", y = "Utah Chill Units", caption = "data for 2017 is exlcuded due to sensor error", colour = "Year") +
		scale_x_continuous(breaks = doy_ticks, labels = doy_labels) +
		theme_bw() + 
		theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
		ylim(-200, 2000)

```

### Dynamic model

Example of the Israel Dynamic model using Te Puke data

```{r, echo=FALSE}

met_data %>%
	filter(StationID == "TPK" & year(date) != 2017) %>%
	ggplot() +
		geom_line(aes(doy, chilling_portions, colour = factor(year))) +
		labs(x = "Date", y = "Chilling portions", colour = "Year", caption = "data for 2017 is excluded due to sensor error") + 
		scale_x_continuous(breaks = doy_ticks, labels = doy_labels) +
		theme_bw() + 
		theme(axis.text.x = element_text(angle = 45, hjust= 1, vjust= 1))

```


### Model comparison

Tabulated chilling units per year, per site for each method

```{r, echo = FALSE, message=FALSE}

yearly_summary <- 
	met_data %>%
	group_by(StationID, year) %>%
	filter(year != 2003) %>%
	summarise(under_seven = max(hours_below_seven),
			  richardson = max(richardson_units),
			  utah = max(utah_units),
			  portions = max(chilling_portions)) 

yearly_summary %>%
	knitr::kable(digits = 2)

```

typical ranges for  each site.

```{r, echo=FALSE, message=FALSE}

model_ranges <- 
	yearly_summary %>%
	group_by(StationID) %>%
	summarise(under_seven = range(under_seven),
			  richardson = range(richardson),
			  utah = range(utah),
			  portions = range(portions)) %>%
	mutate(min_or_max = c("min", "max")) %>%
	pivot_longer(cols = 2:5, names_to = "model_name", values_to = "units") %>% 
	arrange(model_name) %>%
	pivot_wider(id_cols = StationID, names_from = c(model_name, min_or_max), values_from = units)
	
model_ranges <- setNames(model_ranges, c("StationID", rep(c("min", "max"), 4)))

model_ranges %>%
	kbl(digits = 2) %>%
	kable_styling() %>%
	add_header_above(c("", "Chilling portions" = 2, "Richardson units" = 2, "Under 7C" = 2, "Utah units" = 2))

```


```{r scratch, echo=FALSE, eval=FALSE}

excel_calc <- 
	read_csv(here("input/dynamic_all.csv")) 

options(digits = 15)

tmp <- getChillingPortions(excel_calc$`Temp(C)`)

# diffs are occuring at ftmp step, due to excel limiting precision.
# excel limits to 15 decimal places
dynamic_diffs <- data.frame(tk = tmp$tk - excel_calc$`Temp (K)`,
							ftmprt = tmp$ftmprt - excel_calc$ftmprt,
							sr = tmp$sr - excel_calc$sr,
							xi = tmp$xi - excel_calc$xi,
							xs = tmp$xs - excel_calc$xs,
							ak = tmp$ak - excel_calc$ak1,
							inters = tmp$inters - excel_calc$`Inter-S`,
							intere = tmp$intere - excel_calc$`Inter-E`,
							delta = tmp$delta - excel_calc$delt,
							portions = cumsum(tmp$delta) - excel_calc$Portions)

```